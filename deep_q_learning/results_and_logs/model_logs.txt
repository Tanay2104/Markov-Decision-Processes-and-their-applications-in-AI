This files contains the trained model(s) parameters and hyperparameters
Model hyperparameters
Learning Rate: 0.0001, Epochs: 3000, Gamma: 0.995, Batch Size: 64 Initial Epsilon: 1.0, Final Epsilon: 0.01, epsilon decay rate: 0.99997
Layer 0 weights:
 [[ 9.74025463e-01  3.18562164e-01 -2.20013210e+00  1.85230456e-01
  -1.94058469e+00  1.00850399e+00 -1.43391120e+00 -4.99579575e-01
   3.45205316e-01  3.39205647e-02  1.14146064e-01  5.00110570e-01
  -1.87538338e-01 -1.23426991e+00  2.23393042e+00 -3.07965906e-01
  -4.02111744e-01  9.65230104e-01 -8.19865579e-01  8.54382092e-02
   1.48291500e-01 -9.76604388e-01  1.75818485e+00 -1.91920494e-01
   1.82216786e+00 -7.70314404e-01  1.50556330e+00 -1.48084645e+00
   2.20597543e-01 -8.86855560e-02 -2.66784524e+00  6.58762030e-01
   5.98073256e-01 -5.21602130e-01  1.62446061e+00  1.74867798e+00
   1.44391247e+00  1.87444766e-01  7.97020819e-01  1.05325034e+00
   1.32720791e+00 -6.74008254e-01  1.04338272e-01  5.84635562e-01
  -1.92330347e+00 -7.41325440e-01  7.68011146e-01 -1.24080272e+00
  -1.62287311e-01  5.39172919e-01  8.11851617e-01  2.44793496e-01
   2.45552180e-01 -1.06726737e+00  3.41365088e-01 -6.38120525e-01
  -1.02397894e+00  8.45356203e-01  1.17069028e+00  1.52782118e+00
   5.23622563e-01 -8.67488385e-03 -6.84204391e-02 -1.26623722e+00]
 [ 1.32803175e+00 -8.50418164e-01  7.71561135e-01  1.05450261e+00
   4.23507441e-01  1.75255046e+00 -5.66492970e-01 -6.53723561e-01
  -4.54798861e-01 -1.48975047e-01  1.03450742e+00  1.49747282e+00
   7.77565719e-01 -1.36373641e-01 -1.16162230e+00  2.86654736e-01
   5.69322776e-01  1.06491470e+00  2.81274666e+00 -8.06707021e-01
  -1.32334255e+00 -7.36099184e-01 -2.65839272e-01  1.24232642e+00
   2.09900481e+00  6.51801249e-02 -1.65681664e+00  3.23292761e-02
   2.80308978e-01  2.67262477e+00  1.48337094e+00  1.26976374e+00
  -1.31633354e-01 -9.87105480e-01 -1.94048593e+00 -1.60126724e-01
  -6.02574595e-01 -2.95253724e-01 -1.36740321e+00  1.16170252e+00
  -5.55623475e-01 -1.37870064e+00 -7.62849200e-02  5.60701724e-01
  -1.41426035e-01 -1.70698097e+00 -7.89639083e-01  6.32726227e-01
  -1.31512299e+00  1.17612115e+00  1.56103129e-01 -4.02477014e-01
   6.29273817e-01 -6.10409649e-01 -1.89517304e+00 -7.21019427e-01
   1.16625204e+00  3.36212778e-01  1.24835207e+00  7.79542688e-01
   1.45288702e+00 -4.47710095e-02 -2.14280118e-01  1.06371345e+00]
 [ 3.28543884e-01 -6.44637771e-01 -2.48155350e+00 -8.71286134e-01
  -1.32924922e-01  2.86764375e-01 -5.09277522e-01 -2.88512197e-01
   5.01664988e-01  1.67921774e+00  2.42952948e-01 -2.03481659e-01
   8.01526687e-01 -3.55062430e-01 -1.98892110e+00 -1.43015335e-01
   2.41175568e-01  2.02104212e-01  4.25549058e-01  2.62161252e-01
   6.99431469e-01  4.13265020e-01  7.42550796e-01  5.89883884e-01
   3.69016889e-01 -6.59009073e-01  5.57885098e-02  1.30219499e+00
   9.25200664e-02  8.44386225e-01  9.24053750e-01  7.58532805e-01
  -2.03257199e+00 -2.32282055e-01  6.83132323e-02  1.24816276e+00
   1.84572395e+00 -7.25374747e-01  5.54585199e-01  2.77012395e-01
  -6.10245238e-01  1.77281505e+00  6.66256537e-01 -2.59703028e+00
  -3.17931614e-02 -1.31598668e-02  1.24303369e+00  9.54446713e-01
   1.48766926e+00 -2.57332706e-01  8.87273719e-01  1.10681976e-01
  -9.68103535e-01  7.02705962e-01 -1.07272269e+00 -1.66441249e-01
  -6.51090419e-03  1.16597677e+00 -5.60612709e-01  2.71253239e-01
  -6.53662309e-01  1.04734374e+00 -8.58187602e-01  1.41848852e+00]
 [ 9.13621882e-01 -1.04315192e+00 -1.47368633e-01 -3.63637580e-01
   6.97083410e-01 -1.92880557e-01 -9.92927289e-01  5.03330628e-01
   1.95332799e+00  1.90047925e+00  9.65846313e-01  8.89157923e-01
   1.41161535e+00  6.41227723e-01  5.64053309e-01 -1.91298695e+00
   3.41994542e-01 -8.05821035e-01  3.75399851e-01  7.91064708e-01
   7.94271673e-01 -1.40431363e+00 -1.42281188e-02 -4.06103316e-01
  -1.29646665e+00  4.72726327e-01  1.86980050e+00  9.66691820e-01
  -3.62166387e-01 -1.63560266e+00  3.08658674e-01 -2.05656275e+00
   2.10536425e+00 -1.52055320e+00  4.83532953e-01 -1.22722950e+00
   1.33647079e+00  1.28708285e+00 -4.93368087e-01 -1.15575734e+00
  -1.67707435e-01 -5.36520057e-01  6.53365592e-01 -1.20674624e+00
   1.96265674e+00  1.11215070e+00 -1.67968024e+00  2.40561848e-01
   1.84903030e-01  4.60894835e-01 -1.18356894e+00  1.34001260e+00
   7.75175617e-02  1.12182779e-01  1.20450094e+00 -1.17739313e+00
   1.95454966e+00 -1.93235268e+00 -1.75683707e-01 -8.37606273e-01
  -7.09946939e-01  2.56938993e+00 -2.10725381e+00 -3.53163274e-01]
 [-1.97857611e-01  6.71627311e-02  1.84367965e+00 -1.70637375e+00
  -1.14970302e+00  8.74145280e-01 -5.81205228e-01 -3.90927719e-01
  -7.17790677e-01 -4.81369278e-01  5.36984932e-01 -1.66670836e+00
   1.85599415e+00  3.02450214e-01 -1.42103679e+00 -4.95595039e-01
   5.08261491e-01 -9.78995085e-01  3.37730932e-01  1.30972276e+00
  -5.81017384e-01 -3.44159032e-01 -1.30708545e+00  1.16837999e+00
  -4.87409683e-01 -4.26763917e-01 -3.48322928e-01  4.14400139e-01
   1.09727896e+00  1.47663436e+00  4.83420370e-01  8.61132037e-01
  -3.79414852e+00  2.26570982e-01  5.99047013e-01 -1.07648059e+00
   2.14110900e-01  8.63028354e-01 -1.27437811e+00 -3.98476857e-01
  -9.10852834e-01  1.29819322e-03  2.55026459e+00 -4.71243075e-01
   2.56744842e-01 -1.05474771e+00 -1.87838362e+00  3.56246555e-01
   1.59446343e+00  3.38777634e-01  3.17702747e+00  3.55514656e-01
   6.24809546e-01 -5.41235245e-01 -9.46858862e-01 -2.69397339e-01
   3.42172841e-01 -1.37958311e-01  9.77056747e-01 -3.77227965e-01
  -5.00146027e-01  1.55256417e-01 -6.16187850e-01 -5.96175152e-01]
 [-3.60575761e-01 -1.73951084e-01 -2.96280027e-01  6.60225166e-02
  -1.40020757e+00  8.96487680e-01  1.37949425e+00  1.46612325e-01
   3.49528463e-01  6.64012471e-01  9.12689096e-01 -1.01893500e+00
  -1.23117822e+00  1.56962069e+00 -1.05442773e+00 -7.09231689e-01
   1.56950904e+00 -6.85034727e-01  1.64694412e+00 -1.56092112e+00
   7.78223004e-01 -2.04469233e+00 -7.89973974e-01  1.26448293e-01
  -4.38062929e-01 -2.20802545e+00 -2.49426773e-02  1.57629257e+00
   2.97320711e+00 -5.54448045e-01  4.78510902e-01  2.04360423e-01
   3.79725274e-01  1.23615404e+00  6.03305422e-01 -9.53268243e-01
   8.25914439e-01 -3.32281311e-02 -1.82433317e+00 -1.82442913e+00
   6.87802010e-01 -3.21256769e-01 -6.36317500e-01  2.89677373e-01
   1.01596506e+00  1.54848547e+00 -1.35606209e+00 -1.92029845e-02
   8.18617587e-01  1.18670828e+00 -4.81089305e-01 -1.49822953e+00
   1.25251045e+00 -1.30818509e+00 -1.32075605e+00  1.92410908e-02
   7.07388681e-01  1.15552176e+00 -1.43907159e+00 -1.46183200e+00
   1.06597772e-02 -7.60937255e-01  4.87230431e-01  6.96840699e-02]
 [ 1.21583812e+00 -1.59025028e+00 -4.69479585e-01  1.41422843e+00
  -3.70809856e-01  1.19853086e+00 -1.79170918e+00 -1.15420676e+00
   5.30886017e-01 -1.73870902e+00  9.43497937e-01 -1.07679349e+00
  -7.01895044e-01  3.72351516e-01  5.85112227e-02 -8.76182294e-01
   6.85332031e-01 -1.23112311e+00  9.20199817e-02 -1.11805638e+00
  -1.16019784e-01 -8.80145364e-01 -3.57644288e-01  3.61809482e-01
  -8.54699164e-01  3.99310798e-01 -9.34602123e-01  2.59155205e+00
   8.65342256e-01 -1.56080225e+00  1.61720135e+00  9.12686921e-01
  -1.08608428e+00 -5.03499563e-01 -2.91447761e-01  5.13701085e-02
   1.47487975e-01  1.95457458e+00  4.63193682e-01  8.72224199e-01
  -1.11183997e+00 -8.36069472e-01 -8.66866765e-01 -3.19811619e-01
   9.34837478e-01  7.30381920e-01 -6.28692254e-01 -1.37146073e-02
   1.70005273e+00 -5.60925812e-01 -9.31816349e-01  3.39401134e-01
   3.41633200e-01  3.97534667e-01  4.34754756e-01 -6.33320618e-01
   5.59519295e-01  7.90723909e-01 -1.58630036e-01 -9.47125272e-01
  -1.30885433e+00  2.82809608e-01  4.09549811e-01 -5.83221518e-01]
 [-8.92376101e-02 -6.01873353e-01 -4.22056728e-01 -6.46231839e-01
  -3.22169640e+00 -2.04600072e+00 -1.03341230e+00 -1.41155115e+00
   1.27344119e+00  4.67582232e-01  1.17712987e-01  1.71308896e-01
  -4.96345428e-01 -4.59916164e-01 -1.23673598e-01 -1.09782527e+00
   3.93305153e-01 -3.43022222e-01  1.51118662e-01  5.46220286e-01
  -4.48356546e-01 -3.03403974e-01 -1.17600537e+00  2.88495759e-02
   4.65676198e-02 -6.10449487e-03 -5.57455196e-01 -5.75112033e-01
  -4.25751413e-01 -3.43320585e-01  4.84864454e-01 -2.68820880e-01
  -3.24679348e-01 -1.09849719e-01 -1.12526312e+00 -1.15206174e+00
   8.29673506e-01 -1.89878883e+00 -3.38999391e-01 -1.01160735e+00
  -1.56738621e-01 -8.15295664e-01 -1.88167202e+00  6.99635801e-02
  -4.94759103e-01 -6.37819232e-01  1.39895596e+00  6.72240325e-01
   2.45361621e-01  2.18095024e+00  3.51299624e-01  1.69942777e+00
   1.41982581e+00 -4.15949343e-01  3.00600837e-01 -5.53573688e-02
   8.91431448e-02  1.75678633e+00 -1.34661046e+00  1.05227133e+00
  -7.96219370e-01  1.06101527e-01 -8.70791573e-01  5.45359732e-01]]Layer 0 biases:
 [ 0.71889712 -0.38542731  0.72041978  0.55482983 -0.20541213 -0.15097914
  0.82714725 -0.40487761 -0.65268466  0.02532706  0.20511481  0.85701939
  0.27287953 -0.45549404  0.73437466  1.44856119  0.66239754  0.18865548
  0.29191016 -0.34767112 -0.55646767  0.33161988  0.11502064  0.12440082
 -0.15505491 -1.13981512 -0.47444516 -1.19439216 -0.41325033 -0.57851397
  0.73474429  0.29862268  0.18550567 -0.60652939 -0.06762556  0.61432443
 -0.59657927 -0.61040307  0.44708495  0.87454972 -0.67711732 -0.63151604
  0.10763768  0.12649994 -0.15397471  0.25667531 -0.81763445  1.69618371
  0.33684549 -1.2156843   0.27690434  0.61918243 -0.20041707 -0.72751423
 -0.3035607   1.49783058 -0.20829912  0.70842924  1.00725529 -0.03207273
 -0.27558647  0.08369701  1.89985823 -0.06031065]Layer 0 has activation function ReLULayer 1 weights:
 [[ 0.25805631 -1.34775381 -0.14193844 ...  0.87853309 -1.52601486
  -0.1509079 ]
 [-1.10078255 -1.51465339  0.19908355 ...  0.45977737 -0.20338481
  -0.27282729]
 [ 2.0070338   0.36661861  0.15363225 ... -0.77000917  0.38002405
  -1.15046006]
 ...
 [ 0.00325776 -1.31617702 -0.28866378 ...  0.45008179  0.97144592
   0.58155874]
 [-0.65450551  1.53470421  2.08449461 ... -0.04201265 -0.60702078
  -0.29617969]
 [ 1.11060836  1.80696158  0.22684383 ...  0.84969664 -0.5680873
   0.70991035]]Layer 1 biases:
 [-0.07825578 -0.00778577  0.03775887 -0.08334387  0.1383373   0.00852386
 -0.03542117  0.0972454   0.14795829 -0.14693574 -0.04212019 -0.0007886
 -0.06163936  0.00865274  0.10726212  0.01704698 -0.00740322  0.10064356
  0.04195958 -0.13720399  0.09454205 -0.16392718  0.04458393 -0.02842786
  0.00459133  0.0357605  -0.01765997  0.11852289 -0.03418232 -0.04887371
 -0.00270922 -0.04780669  0.14123788 -0.09177808  0.08519481 -0.11946988
  0.14010933 -0.07619104 -0.00675199  0.03988446 -0.02936528  0.151149
 -0.00861206  0.07830561  0.0161408  -0.03556097  0.12134325  0.10806114
 -0.07600086 -0.03538472  0.06757187  0.22519483 -0.24493065 -0.13310179
  0.02581218 -0.00796864 -0.02513692 -0.00993314  0.10045281  0.04566558
 -0.03027094  0.0269091   0.06765332  0.12375854]Layer 1 has activation function ReLULayer 2 weights:
 [[-0.50187233 -1.84515833 -1.65148643 -1.26465979]
 [-1.5667619   0.29046219  0.32151716 -0.77330175]
 [ 0.06201549 -0.07132603  0.32375583 -0.52113504]
 [ 0.81314684 -1.28247307 -0.1874258   0.37193134]
 [ 1.22083378  1.70643638  2.02063315  1.04740433]
 [ 1.55761175  0.28576916 -0.44045135  0.07206721]
 [-0.4339474  -1.69671206  0.56918018 -1.21535661]
 [-1.38657711 -1.58086042 -2.33397518 -0.3599102 ]
 [-0.15506615 -2.16459169 -0.32339832 -1.4840039 ]
 [-2.54108314 -1.81586731 -2.36185293 -1.25637369]
 [ 0.02845889 -0.62325854  0.63362459  0.24307775]
 [ 0.18953801  0.57933251 -0.19710164  0.14430305]
 [ 0.3511244   1.4780458   1.27937085 -0.11173562]
 [-1.29106997  1.08656723  1.32316164  0.58722283]
 [ 0.22730898 -1.92328056 -1.50325816  0.61330638]
 [ 0.1481564   2.11084382  1.71381733  0.66768978]
 [-0.82194934  0.33804945  1.01058293  0.40877906]
 [ 0.84078875  1.22733462  1.7330379   0.05519695]
 [-0.67747926  0.22461158  0.1862616  -0.5346644 ]
 [ 2.21255967  2.16905167  2.08425555  1.83734334]
 [ 0.41911166  0.95267132  0.46423922 -0.61750668]
 [ 3.73923444  2.02590727  0.68139944  1.5110984 ]
 [ 0.46564636  0.45417441  0.54664899  0.31200904]
 [ 2.12162584 -1.23403561 -1.27648789 -0.20823535]
 [ 1.15810389  1.59244754 -1.53411508 -0.62216049]
 [ 0.4887687  -1.28124025  0.7566272  -0.38671946]
 [-0.43875664 -0.07174124  0.70164859 -0.98190514]
 [-0.80834004  0.98978765 -0.7486928  -1.30651214]
 [ 0.38323419  0.41295461  0.03195254  0.74291676]
 [ 0.91213179 -2.13062935 -1.21881158 -1.2435519 ]
 [ 0.65832673 -1.23603863 -1.08884868 -0.0953374 ]
 [-2.44558394  0.65374144 -0.27000181 -0.54380116]
 [ 2.29657772  0.90497145  1.1974241   1.30432166]
 [ 0.07190881 -1.18191397  0.44428221  0.43117535]
 [ 0.05916684  2.03581124 -0.34773324  1.87055432]
 [ 1.32775025  1.84542537  1.3532196   2.12230441]
 [-1.14131055 -0.44162066 -0.6378138  -1.58246946]
 [-1.80632917 -0.69212072 -1.71813968 -1.33223567]
 [ 0.13193786  1.49075742  1.79893384  1.12172668]
 [-0.74823036  0.3469259   2.30082514 -0.60785641]
 [-2.12592336  0.49007015  1.23891016 -1.52196467]
 [ 0.58149998  0.00726507  1.57990539 -0.96169204]
 [-1.26508486  0.58207542  1.2506532  -1.11464523]
 [-0.46342268 -0.06291534 -0.57716415 -0.90544496]
 [-0.03472867  0.19781999  0.04532851  0.83422238]
 [ 1.27830683 -1.31940706 -1.32873844 -0.4276806 ]
 [ 2.1243579   1.20963588  1.53484464  2.53362436]
 [ 1.08162541  0.48375877  2.14724176  1.64421447]
 [ 1.43365401 -0.50850332 -1.5258344   1.78800927]
 [ 0.45929935 -0.83269252 -0.22686418  0.1326914 ]
 [-0.14123628  0.91627628 -0.41166542  0.11856255]
 [ 0.74378564  1.69140246  0.16614287  1.52553463]
 [-0.17893717 -1.41956644  0.8907127   1.17444906]
 [ 0.48265437  0.75554294 -0.14530434 -1.46507614]
 [-1.2156052  -0.05062102 -1.66076372  0.47146069]
 [ 2.93794796  0.71569329  0.94945475  2.01371752]
 [-0.21721719  0.08839109  0.66558545  0.99675248]
 [-1.4359479   0.11142776  0.61651168 -1.42465336]
 [-0.64463066  0.95768865  0.2913466   1.03660504]
 [ 0.09950272  0.90521047  0.86304812  1.85427745]
 [-1.29817489  0.31836852  0.29179353  1.12775574]
 [-0.06398076  0.06858365  0.23618799  0.8076438 ]
 [ 0.48641676  0.36838576  0.66264305  1.59669434]
 [ 0.39775665  1.14662743  1.25505962  0.32901301]]Layer 2 biases:
 [ 0.04819411  0.02312617  0.04828175 -0.00635167]Layer 2 has activation function Linear-----
